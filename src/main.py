# src/main.py
# -*- coding: utf-8 -*-
"""
TXT â†’ Markdown ë³€í™˜ê¸° (v0.1.1 + optional LLM assist)
- ê¸°ë³¸: ê·œì¹™ ê¸°ë°˜(tagging_rules.yaml)ìœ¼ë¡œ ì„¹ì…˜ ë¶„ë¥˜/ìš”ì•½
- ì˜µì…˜: --llm ì¼œë©´ LLM ìš”ì•½ìœ¼ë¡œ ì„¹ì…˜ì„ ë³´ê°•(ì‹¤íŒ¨/ë¯¸ì§€ì› ì‹œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ í´ë°±)
- ë¯¼ê°ì •ë³´ ë§ˆìŠ¤í‚¹(--mask), ê³ ê°ì‚¬ëª…/í‚¤ì›Œë“œ íƒœê¹…(customers.yaml / tagging_rules.yaml) ì§€ì›
"""

import argparse
import os
import re
import sys
import json
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional

try:
    import yaml
except ImportError:
    print("[ERROR] PyYAMLì´ í•„ìš”í•©ë‹ˆë‹¤. `pip install pyyaml` í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.", file=sys.stderr)
    sys.exit(1)


# ---------- ìœ í‹¸ ----------

def load_yaml(path: str) -> dict:
    p = Path(path)
    if not p.exists():
        return {}
    with p.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def read_text(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8", errors="ignore")
    except Exception as e:
        print(f"[WARN] Read failed: {path} ({e})")
        return ""


def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)


def mask_ip_mac(text: str) -> str:
    # IPv4
    text = re.sub(r"\b(?:(?:25[0-5]|2[0-4]\d|[01]?\d?\d)\.){3}(?:25[0-5]|2[0-4]\d|[01]?\d?\d)\b",
                  "[REDACTED_IP]", text)
    # MAC
    text = re.sub(r"\b(?:[0-9A-Fa-f]{2}[:\-]){5}[0-9A-Fa-f]{2}\b", "[REDACTED_MAC]", text)
    return text


def mask_customers(text: str, customers_cfg: dict) -> str:
    """
    customers.yaml ì˜ˆì‹œ:
    patterns:
      - "ì¼€í”¼ì½”"
      - "KEFICO"
      - "SK ?hynix"
      - "í•˜ì´ë‹‰ìŠ¤"
    """
    pats = (customers_cfg.get("patterns") or []) if isinstance(customers_cfg, dict) else []
    for pat in pats:
        try:
            text = re.sub(pat, _mask_match, text, flags=re.IGNORECASE)
        except re.error:
            # ì •ê·œì‹ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ì›ë¬¸ ìœ ì§€
            continue
    return text


def _mask_match(m: re.Match) -> str:
    s = m.group(0)
    if len(s) <= 2:
        return "*" * len(s)
    # ê°€ìš´ë° ë§ˆìŠ¤í‚¹
    return s[0] + ("*" * (len(s) - 2)) + s[-1]


def infer_title(src_path: Path, text: str) -> str:
    # 1) íŒŒì¼ ì²« ì¤„ì´ ì œëª©ì²˜ëŸ¼ ë³´ì´ë©´ ì‚¬ìš©
    first_line = (text.splitlines() or [""])[0].strip("# ").strip()
    if 3 <= len(first_line) <= 120:
        return first_line
    # 2) íŒŒì¼ëª… ê¸°ë°˜
    return src_path.stem[:120]


def dedup_keep_order(items: List[str]) -> List[str]:
    seen = set()
    out = []
    for x in items:
        k = x.strip()
        if not k:
            continue
        if k not in seen:
            seen.add(k)
            out.append(k)
    return out


def json_array(items: List[str]) -> str:
    return json.dumps(items, ensure_ascii=False)


# ---------- ê·œì¹™ ê¸°ë°˜ ì„¹ì…˜ ë¶„ë¥˜ ----------

def section_signals_from_rules(rules: dict) -> dict:
    # tagging_rules.yaml ì•ˆì— section_signals ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    # ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
    default = {
        "summary": ["ì¦ìƒ", "í˜„ìƒ", "ë¬¸ì œ", "ì˜¤ë¥˜", "ì—ëŸ¬", "ì¥ì• ", "ë¡œê·¸", "ë°œìƒ", "í™˜ê²½", "ë²„ì „"],
        "root_cause": ["ì›ì¸", "ê·¼ë³¸ ì›ì¸", "ë¶„ì„", "ì¬í˜„", "because", "due to", "ì •ì±… ì¶©ëŒ", "ì„¸ì…˜", "íƒ€ì„ì•„ì›ƒ", "DHCP", "DNS", "ARP", "TTL", "ì˜¤íƒ"],
        "actions": ["ì¡°ì¹˜", "í•´ê²°", "ìˆ˜ì •", "ë³€ê²½", "ì ìš©", "ì¬ê¸°ë™", "ì¬ì‹œì‘", "ëª…ë ¹ì–´", "ì¿¼ë¦¬", "ìŠ¤í¬ë¦½íŠ¸", "ì„¤ì •ê°’", "íŒ¨ì¹˜"],
        "prevention": ["ì¬ë°œ ë°©ì§€", "SOP", "ëª¨ë‹ˆí„°ë§", "ì•ŒëŒ", "ìš´ì˜ ê¸°ì¤€", "ì²´í¬ë¦¬ìŠ¤íŠ¸", "ìë™í™”", "ê¶Œì¥ê°’", "íŠœë‹", "í•œê³„", "ì£¼ì˜"]
    }
    if isinstance(rules, dict) and "section_signals" in rules:
        return rules["section_signals"] or default
    return default


def score_sentence(sent: str, signals: dict) -> str:
    s = sent.lower()
    # ê°€ì¤‘ì¹˜ ê¸°ë³¸
    scores = {k: 0 for k in ["summary", "root_cause", "actions", "prevention"]}

    # í‚¤ì›Œë“œ ë§¤ì¹­
    for sec, words in signals.items():
        for w in words:
            try:
                if re.search(w, s, flags=re.IGNORECASE):
                    scores[sec] += 1
            except re.error:
                # ì‹ í˜¸ì–´ê°€ ì •ê·œì‹ì¸ë° ì˜¤ë¥˜ì¼ ê²½ìš° ë¬´ì‹œ
                continue

    # ì¶”ê°€ íœ´ë¦¬ìŠ¤í‹±
    if re.search(r"```|^\s{0,4}(\$|sudo|systemctl|msiexec|mysql|curl|kubectl|helm|terraform)\b", sent, re.IGNORECASE | re.MULTILINE):
        scores["actions"] += 2
    if re.search(r"\b(err(or)?|fail(ed)?|exception|traceback|timeout|denied)\b", s):
        scores["root_cause"] += 1
    if re.search(r"(ê¶Œì¥|ì£¼ì˜|ëª¨ë‹ˆí„°ë§|ì£¼ê¸°|ì„ê³„ì¹˜|threshold|alert)", s):
        scores["prevention"] += 1

    # ìƒë‹¨ë¶€ ê°€ì‚°(ì´ˆë°˜ ëª‡ ì¤„ì€ ìƒí™©ìš”ì•½ì¼ í™•ë¥ â†‘)
    # (ì´ í•¨ìˆ˜ ë‹¨ë…ìœ¼ë¡œëŠ” ìœ„ì¹˜ì •ë³´ê°€ ì—†ìœ¼ë‹ˆ ìƒìœ„ì—ì„œ ì¼ë¶€ë§Œ summaryë¡œ ê³ ì •í•˜ëŠ” ì „ëµì„ ì¨ë„ ë¨)
    # ì—¬ê¸°ì„œëŠ” ìƒëµ.

    # ìµœëŒ€ ë“ì  ì„¹ì…˜
    section = max(scores, key=lambda k: scores[k])
    return section


def classify_sections_rule_based(text: str, rules: dict) -> Dict[str, str]:
    """
    ê°„ë‹¨: ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìŠ¤ì½”ì–´ â†’ ì„¹ì…˜ ë°°ì¹˜
    """
    signals = section_signals_from_rules(rules)
    paras = [p.strip() for p in re.split(r"\n\s*\n", text) if p.strip()]
    buckets = {"summary": [], "root_cause": [], "actions": [], "prevention": []}

    # ë§¨ ì• 10ì¤„ ì •ë„ëŠ” summary ê°€ì‚°(ë¬¸ë‹¨ ë‹¨ìœ„ë¼ ê°„ë‹¨íˆ ì²˜ìŒ 1~2ê°œë¥¼ summaryì— ìš°ì„  ë°°ì¹˜)
    if paras:
        head = paras[:2]
        for p in head:
            buckets["summary"].append(p)
        body = paras[2:]
    else:
        body = []

    for p in body:
        sec = score_sentence(p, signals)
        buckets[sec].append(p)

    # ì„¹ì…˜ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    out = {k: "\n\n".join(v).strip() for k, v in buckets.items()}
    # ë„ˆë¬´ ì§§ìœ¼ë©´ ë¹ˆ ê°’ ì²˜ë¦¬(ë¹ˆ ì„¹ì…˜ì€ í…œí”Œë¦¿ì—ì„œ ì‚¬ì‹¤ìƒ ìŠ¤í‚µ)
    for k, v in out.items():
        if len(v) < 20:
            out[k] = ""
    return out


# ---------- í…œí”Œë¦¿ ë Œë” ----------

def render_markdown(template_path: str, fm: Dict[str, str], body: Dict[str, str]) -> str:
    """
    template.md.tpl ì— ë‹¤ìŒ í”Œë ˆì´ìŠ¤í™€ë”ê°€ ìˆë‹¤ê³  ê°€ì •:
      {title} {category_json} {tags_json} {date} {source_file} {storage_category}
      {summary} {root_cause} {actions} {prevention} {summary_tldr}
    """
    tpl = Path(template_path).read_text(encoding="utf-8")
    data = {
        "title": fm.get("title", ""),
        "category_json": json_array(fm.get("category", [])),
        "tags_json": json_array(fm.get("tags", [])),
        "publish": str(fm.get("publish", True)).lower(),
        "date": fm.get("date", ""),
        "source_file": fm.get("source_file", ""),
        "storage_category": fm.get("storage_category", ""),
        "summary_tldr": body.get("summary_tldr", "").strip(),
        "summary": body.get("summary", "").strip(),
        "root_cause": body.get("root_cause", "").strip(),
        "actions": body.get("actions", "").strip(),
        "prevention": body.get("prevention", "").strip(),
        "raw_body": body.get("raw_body", "").strip(),
    }
    out = tpl
    for k, v in data.items():
        out = out.replace("{" + k + "}", v)

    # ë¹ˆ ì„¹ì…˜ í—¤ë” ì •ë¦¬(ì•„ì£¼ ë‹¨ìˆœí•œ í›„ì²˜ë¦¬)
    out = _strip_empty_section(out, "## ğŸ§­ ìƒí™© ìš”ì•½ (What happened)", data["summary"] or data["summary_tldr"])
    out = _strip_empty_section(out, "## ğŸ§  ì›ì¸ ë¶„ì„ (Root cause)", data["root_cause"])
    out = _strip_empty_section(out, "## ğŸ›  ì¡°ì¹˜ ë°©ì•ˆ (Action taken)", data["actions"])
    out = _strip_empty_section(out, "## ğŸ” ì¬ë°œ ë°©ì§€ / ìš´ì˜ ê°€ì´ë“œ (Prevention / SOP)", data["prevention"])
    return out


def _strip_empty_section(md: str, header: str, content: str) -> str:
    if content:
        return md
    # í—¤ë” ë¼ì¸ë¶€í„° ë‹¤ìŒ í—¤ë” ì „ê¹Œì§€ ì‚­ì œ (ê°„ë‹¨ íŒ¨í„´)
    pattern = rf"{re.escape(header)}\n.*?(?=\n## |\Z)"
    return re.sub(pattern, "", md, flags=re.DOTALL)


# ---------- LLM ë³´ì¡° (ì˜µì…˜) ----------

def try_llm_extract(raw_text: str, llm_cfg: dict) -> Dict[str, str]:
    """
    llm_summarizer.extract_sections() í˜¸ì¶œ.
    - í‚¤/ì„¤ì •/í”„ë¡¬í”„íŠ¸ ì—†ìœ¼ë©´ {} ë°˜í™˜ â†’ ìƒìœ„ì—ì„œ ê·œì¹™ ê¸°ë°˜ ê²°ê³¼ ì‚¬ìš©
    """
    if not llm_cfg:
        return {}
    # ê¸¸ì´ ê¸°ì¤€
    min_len = int(llm_cfg.get("pass_through_if_shorter_than", 0))
    if len(raw_text) < min_len:
        return {}

    try:
        from llm_summarizer import extract_sections
    except Exception:
        return {}

    try:
        out = extract_sections(raw_text, llm_cfg) or {}
        # ìŠ¤í‚¤ë§ˆ í‚¤ë¥¼ ê°•ì œ ë³´ì •
        return {
            "title": out.get("title") or "",
            "summary_tldr": out.get("summary_tldr") or "",
            "summary": out.get("summary") or "",
            "root_cause": out.get("root_cause") or "",
            "actions": out.get("actions") or "",
            "prevention": out.get("prevention") or "",
            "tags": out.get("tags") or [],
            "category": out.get("category") or [],
        }
    except Exception:
        return {}


# ---------- ë©”ì¸ ----------

def main():
    ap = argparse.ArgumentParser(description="TXT â†’ MD converter (rule-based, optional LLM assist)")
    ap.add_argument("--src_dir", required=True, help="ì›ë³¸ TXT ë£¨íŠ¸ ë””ë ‰í† ë¦¬")
    ap.add_argument("--out_dir", required=True, help="ê²°ê³¼ MD ë£¨íŠ¸ ë””ë ‰í† ë¦¬(Obsidian Vault í•˜ìœ„ ê¶Œì¥)")
    ap.add_argument("--customers", default="./configs/customers.yaml", help="ê³ ê°ì‚¬ ë§ˆìŠ¤í‚¹ ì„¤ì •")
    ap.add_argument("--rules", default="./configs/tagging_rules.yaml", help="íƒœê¹…/ì„¹ì…˜ ê·œì¹™")
    ap.add_argument("--template", default="./templates/template.md.tpl", help="Markdown í…œí”Œë¦¿")
    ap.add_argument("--mask", action="store_true", help="IP/MAC/ê³ ê°ì‚¬ ë§ˆìŠ¤í‚¹ í™œì„±í™”")
    ap.add_argument("--dry_run", action="store_true", help="íŒŒì¼ ì €ì¥ ì—†ì´ ì²˜ë¦¬ë§Œ")
    # LLM ì˜µì…˜(ì‹¤í—˜)
    ap.add_argument("--llm", action="store_true", help="LLM ë³´ì¡° ìš”ì•½ í™œì„±í™” (ì‹¤íŒ¨ì‹œ ê·œì¹™ ê¸°ë°˜ í´ë°±)")
    ap.add_argument("--llm-config", default="./configs/llm_config.yaml", help="LLM ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    args = ap.parse_args()

    src_dir = Path(os.path.expanduser(args.src_dir)).resolve()
    out_dir = Path(os.path.expanduser(args.out_dir)).resolve()

    if not src_dir.exists():
        print(f"[ERROR] src_dir not found: {src_dir}", file=sys.stderr)
        sys.exit(2)

    ensure_dir(out_dir)

    customers_cfg = load_yaml(args.customers)
    rules_cfg = load_yaml(args.rules)

    # LLM ì„¤ì • ë¡œë“œ(ì˜µì…˜)
    llm_cfg = {}
    if args.llm:
        llm_cfg = load_yaml(args.llm_config) or {}
        # ì„¤ì •ì´ ì—†ìœ¼ë©´ ì¡°ìš©íˆ ê·œì¹™ ê¸°ë°˜ë§Œ ì‚¬ìš©
        if not llm_cfg:
            print("[WARN] LLM ì„¤ì •ì´ ì—†ì–´ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (--llm-config í™•ì¸).")

    txt_files: List[Path] = []
    for p in src_dir.rglob("*"):
        if p.is_file() and p.suffix.lower() == ".txt":
            txt_files.append(p)

    if not txt_files:
        print(f"[WARN] TXT íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {src_dir}")
        print("ì™„ë£Œ: TXT â†’ MD ë³€í™˜ì´ ëë‚¬ìŠµë‹ˆë‹¤. Obsidian/Notionì— ì—°ê²°í•˜ì„¸ìš”.")
        return

    total = 0
    written = 0

    for src_path in sorted(txt_files):
        total += 1
        rel = src_path.relative_to(src_dir)
        dest_path = (out_dir / rel).with_suffix(".md")
        ensure_dir(dest_path.parent)

        raw = read_text(src_path)
        if not raw.strip():
            print(f"[SKIP] Empty file: {src_path}")
            continue

        # ë§ˆìŠ¤í‚¹
        masked = raw
        if args.mask:
            masked = mask_ip_mac(masked)
            masked = mask_customers(masked, customers_cfg)

        # ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
        rb_sections = classify_sections_rule_based(masked, rules_cfg)
        rb_tags = infer_tags(src_path, masked, rules_cfg)
        rb_category = infer_category(src_path, masked, rules_cfg)

        # LLM ë³´ì¡°(ì„±ê³µ ì‹œ í•´ë‹¹ ì„¹ì…˜/íƒœê·¸/ì¹´í…Œê³ ë¦¬ ë³´ê°•)
        llm_out = {}
        if args.llm:
            llm_out = try_llm_extract(masked, llm_cfg)

        # íƒ€ì´í‹€
        title = (llm_out.get("title") or "").strip() if llm_out else ""
        if not title:
            title = infer_title(src_path, masked)

        # í•©ì„±(LLM ìš°ì„  â†’ ê·œì¹™ ê¸°ë°˜ í´ë°±)
        final = {
            "summary_tldr": (llm_out.get("summary_tldr") or "").strip(),
            "summary": (llm_out.get("summary") or "").strip() or rb_sections.get("summary", ""),
            "root_cause": (llm_out.get("root_cause") or "").strip() or rb_sections.get("root_cause", ""),
            "actions": (llm_out.get("actions") or "").strip() or rb_sections.get("actions", ""),
            "prevention": (llm_out.get("prevention") or "").strip() or rb_sections.get("prevention", ""),
            "raw_body": masked.strip(),
        }

        final_tags = dedup_keep_order((rb_tags or []) + (llm_out.get("tags") or []))[:5]
        final_cat = dedup_keep_order((rb_category or []) + (llm_out.get("category") or []))[:3]

        # Front matter
        fm = {
            "title": title,
            "tags": final_tags,
            "category": final_cat,
            "publish": True,
            "date": datetime.fromtimestamp(src_path.stat().st_mtime).strftime("%Y-%m-%d"),
            "source_file": str(src_path),
            "storage_category": "/".join(rel.parts[:-1]) if len(rel.parts) > 1 else "",
        }

        # í…œí”Œë¦¿ ë Œë”
        md_text = render_markdown(args.template, fm, final)

        if args.dry_run:
            print(f"[DRY] {src_path} -> {dest_path}")
        else:
            dest_path.write_text(md_text, encoding="utf-8")
            written += 1
            print(f"[OK] {src_path} -> {dest_path}")

    print(f"ì™„ë£Œ: TXT â†’ MD ë³€í™˜ì´ ëë‚¬ìŠµë‹ˆë‹¤. Obsidian/Notionì— ì—°ê²°í•˜ì„¸ìš”. (ì´ {total}ê±´, ìƒì„± {written}ê±´)")


# ---------- íƒœê·¸/ì¹´í…Œê³ ë¦¬ ì¶”ì •(ê°„ë‹¨ ê·œì¹™) ----------

def infer_tags(src_path: Path, text: str, rules: dict) -> List[str]:
    """
    tagging_rules.yaml ì˜ˆì‹œ:
    tags:
      NAC: ["NAC","ì„¼ì„œ","ì¸ì¦"]
      Ubuntu: ["Ubuntu","ìš°ë¶„íˆ¬"]
      VM: ["VM","ê°€ìƒí™”","ESXi"]
      Elasticsearch: ["Elasticsearch","ES","ì¸ë±ìŠ¤"]
    """
    out: List[str] = []
    tags_map = (rules.get("tags") or {}) if isinstance(rules, dict) else {}
    hay = (src_path.name + "\n" + text)
    for tag, keys in tags_map.items():
        if not isinstance(keys, list):
            continue
        for k in keys:
            try:
                if re.search(k, hay, flags=re.IGNORECASE):
                    out.append(tag)
                    break
            except re.error:
                continue
    return dedup_keep_order(out)


def infer_category(src_path: Path, text: str, rules: dict) -> List[str]:
    """
    tagging_rules.yaml ì˜ˆì‹œ:
    categories:
      - "NAC"
      - "Ubuntu"
      - "VM"
      - "Elasticsearch"
    íŒŒì¼ëª…/ë³¸ë¬¸ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨ ë§¤í•‘. ë³µìˆ˜ ì¹´í…Œê³ ë¦¬ í—ˆìš©.
    """
    cats = rules.get("categories") or []
    if not isinstance(cats, list):
        return []
    out: List[str] = []
    hay = (src_path.name + "\n" + text)
    for c in cats:
        try:
            if re.search(c, hay, flags=re.IGNORECASE):
                out.append(c)
        except re.error:
            continue
    return dedup_keep_order(out)


if __name__ == "__main__":
    main()
